# Pipeline de IngenierÃ­a de Datos con Arquitectura Medallion

Pipeline completo de ingenierÃ­a de datos que implementa una arquitectura medallion con estadÃ­sticas incrementales O(1) para el procesamiento eficiente de datos de transacciones. El sistema descarga, procesa y analiza archivos CSV usando micro-batches y mantiene estadÃ­sticas en tiempo real sin recalcular desde la base de datos.

<div align="center">
    <p>
        <img src="https://www.python.org/static/community_logos/python-logo.png" alt="Python 3.11" height="60"/>
        <img src="https://airflow.apache.org/images/feature-image.png" alt="Apache Airflow" height="60"/>
        <img src="https://streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.svg" alt="Streamlit" height="60"/>
    </p>
</div>

## ğŸ¤– Desarrollo Asistido por IA
Este proyecto fue desarrollado con la asistencia de Claude (Anthropic) como herramienta de ingenierÃ­a de software, quien contribuyÃ³ significativamente en las pruebas requeridas, implementaciÃ³n de buenas prÃ¡cticas y optimizaciÃ³n del cÃ³digo.

## âœ¨ CaracterÃ­sticas Principales

- ğŸ¥‰ **Arquitectura Medallion**: Bronze layer con formato Parquet optimizado
- âš¡ **Micro-batches**: Procesamiento de 1,000 filas por batch para eficiencia de memoria
- ğŸ“Š **EstadÃ­sticas O(1)**: Motor incremental que no recalcula desde base de datos
- ğŸŒªï¸ **OrquestaciÃ³n**: Apache Airflow para automatizaciÃ³n
- ğŸ“± **Interfaz Web**: Dashboard interactivo con Streamlit
- ğŸ—„ï¸ **SQLite**: Base de datos embebida con esquemas optimizados
- ğŸ” **VerificaciÃ³n**: ComparaciÃ³n automÃ¡tica estadÃ­sticas vs consultas directas

## ğŸ—ï¸ Arquitectura

```mermaid
graph TD
    A[CSV Files] --> B[Google Drive]
    B --> C[Download Script]
    C --> D[Raw CSV Files]
    D --> E[Bronze Converter]
    E --> F[Bronze Layer - Parquet]
    F --> G[Data Ingestion Pipeline]
    G --> H[SQLite Database]
    G --> I[Incremental Stats Engine]
    H --> J[Analytics Dashboard]
    I --> J
    K[Apache Airflow] --> C
    K --> E
    K --> G
    L[Streamlit UI] --> J
```

## ğŸ› ï¸ Requisitos del Sistema

### Software Requerido

- Python 3.11+
- Git
- tmux (para Airflow)

### Dependencias Python

```bash
# Core dependencies
pandas>=2.0.0
pyarrow>=12.0.0
sqlalchemy>=1.4.36
apache-airflow==2.8.4
streamlit==1.45.1
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/pragma-test.git
cd pragma-test
```

### 2. Configurar Virtual Environment

```bash
# Crear virtual environment 
python3.11 -m venv .venv

#Como sugerencia personal usar uv para mayor eficiencia en la instalaciÃ³n
uv venv

# Activar (Linux/Mac)
source .venv/bin/activate

# Activar (Windows)
.venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
uv pip install -r requirements.txt
```

### 4. Configurar Variables de Entorno

```bash
export PYTHONPATH="${PWD}/src:$PYTHONPATH"
export PROJECT_ROOT="${PWD}"
```

## ğŸ“– Formas de Uso

El sistema ofrece tres formas diferentes de ejecutiÃ³n segÃºn tus necesidades, enfoque que ofrece mayor robustez y alternativas para ejecutar este proyecto segÃºn sus necesidades:

### ğŸ–¥ï¸ 1. EjecuciÃ³n por Consola

#### OpciÃ³n A: Pipeline Maestro Completo

```bash
# Ejecutar pipeline completo (script que ejecuta en secuencia otros scripts con distintas tareas)
cd src/pipeline
python master_pipeline.py --batch-size 1000

# Con configuraciones especÃ­ficas
python master_pipeline.py --batch-size 2000 --no-stats
```

#### OpciÃ³n B: Pasos Individuales

```bash
# 1. Descargar datos
cd src/data_flow
python download_data.py

# 2. Convertir a Bronze (Parquet)
python bronze_converter.py

# 3. Ejecutar pipeline principal
cd ../pipeline
python data_ingestion.py
```

#### Scripts de Utilidad

```bash
# Verificar estado
./scripts/check_airflow.sh

# Limpiar datos (CUIDADO: elimina todo)
./scripts/cleanup_airflow.sh
```

### ğŸŒªï¸ 2. OrquestaciÃ³n con Apache Airflow

#### ConfiguraciÃ³n Inicial

```bash
# Instalar y configurar Airflow
./scripts/setup_airflow.sh

# Iniciar servicios
./scripts/start_airflow.sh
```

#### Acceso Web

- URL: http://localhost:8080
- Usuario: admin
- Password: admin123

#### DAGs Disponibles

- **simple_pipeline_execution**
  - Pipeline simplificado que ejecuta scripts directamente
  - Ejecuta archivos principales (2012-1 a 2012-5)
  - Genera reportes automÃ¡ticos

#### GestiÃ³n de Airflow

```bash
# Verificar estado
./scripts/check_airflow.sh

# Parar servicios
./scripts/stop_airflow.sh

# Reiniciar
./scripts/restart_airflow.sh

# Limpieza forzada (si hay problemas)
./scripts/cleanup_airflow.sh
```

### ğŸ“± 3. Interfaz Web con Streamlit

#### Iniciar AplicaciÃ³n

```bash
# Ejecutar dashboard
streamlit run streamlit_app/main.py

# En otra terminal (opcional) - configurar puerto especÃ­fico
streamlit run streamlit_app/main.py --server.port 8501
```

#### Acceso y NavegaciÃ³n

- URL: http://localhost:8501
- Dashboard Principal: Vista general del sistema
- Control Pipeline: Ejecutar y monitorear pipeline
- Explorador: Analizar datos procesados
- Database Viewer: Consultar BD directamente
- VerificaciÃ³n del Reto: Comprobar cumplimiento de requerimientos

#### Funcionalidades Web

##### ğŸ›ï¸ Control del Pipeline

- Ejecutar pipeline completo con configuraciÃ³n personalizada
- Monitoreo en tiempo real con logs
- VerificaciÃ³n de requerimientos del reto
- Limpieza de datos

##### ğŸ“Š Explorador de Datos

- VisualizaciÃ³n de datos Bronze, BD y CSV
- GrÃ¡ficos interactivos con Plotly
- Filtros y anÃ¡lisis estadÃ­stico
- ExportaciÃ³n de resultados

##### ğŸ—„ï¸ Visor de Base de Datos

- Consultas SQL directas
- Explorador visual de tablas
- InformaciÃ³n de esquemas
- EstadÃ­sticas de BD

##### ğŸ§ª VerificaciÃ³n del Reto

- EstadÃ­sticas en ejecuciÃ³n
- Consultas directas a BD
- ComparaciÃ³n antes/despuÃ©s de validation.csv
- VerificaciÃ³n automÃ¡tica de coincidencias

## ğŸ“Š Funcionamiento del Sistema

### 1. Descarga de Datos

```bash
# Descarga automÃ¡tica desde Google Drive
# ExtracciÃ³n y verificaciÃ³n de 6 archivos CSV
```

### 2. ConversiÃ³n Bronze

```bash
# CSV â†’ Parquet con compresiÃ³n snappy
# Micro-batches de 1,000 filas
# Metadatos y validaciÃ³n de esquemas
```

### 3. Ingesta a Base de Datos

```bash
# Procesamiento en micro-batches
# EstadÃ­sticas incrementales O(1)
# VerificaciÃ³n automÃ¡tica
```

### 4. Procesamiento de Validation

```bash
# EstadÃ­sticas ANTES de validation
# Procesamiento de validation.csv
# EstadÃ­sticas DESPUÃ‰S y comparaciÃ³n
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

```bash
# ConfiguraciÃ³n del pipeline
export BATCH_SIZE=1000
export PIPELINE_ENV=production
export AIRFLOW_HOME="${PROJECT_ROOT}/airflow_config"

# Base de datos
export DB_TYPE=sqlite
export DB_PATH="${PROJECT_ROOT}/data/pipeline.db"
```

### Archivos de ConfiguraciÃ³n

#### src/config/pipeline_config.py

```python
PIPELINE_CONFIG = {
    "batch_size": 1000,
    "enable_data_quality_checks": True,
    "enable_statistics_persistence": True
}
```

#### src/config/medallion_config.py

```python
BRONZE_CONFIG = {
    "compression": "snappy",
    "micro_batch_size": 1000,
    "memory_optimization": True
}
```

## ğŸ“ Estructura del Proyecto

```
pragma-test/
â”œâ”€â”€ ğŸ“ src/                         # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ ğŸ“ config/                  # Configuraciones
â”‚   â”œâ”€â”€ ğŸ“ data_flow/               # Descarga y conversiÃ³n
â”‚   â””â”€â”€ ğŸ“ pipeline/                # Pipeline principal
â”œâ”€â”€ ğŸ“ docs/                        # Imagenes referentes al proyecto y documentaciÃ³n
â”œâ”€â”€ ğŸ“ airflow_config/              # ConfiguraciÃ³n Airflow
â”‚   â”œâ”€â”€ ğŸ“ dags/                    # DAGs de Airflow
â”‚   â””â”€â”€ ğŸ“„ airflow.cfg              # ConfiguraciÃ³n
â”œâ”€â”€ ğŸ“ streamlit_app/               # Interfaz web
â”‚   â”œâ”€â”€ ğŸ“„ main.py                  # Dashboard principal
â”‚   â””â”€â”€ ğŸ“ pages/                   # PÃ¡ginas especÃ­ficas
â”œâ”€â”€ ğŸ“ scripts/                     # Scripts sh para AirFlow
â”œâ”€â”€ ğŸ“ data/                        # Datos procesados
â”‚   â”œâ”€â”€ ğŸ“ raw/                     # CSV originales
â”‚   â””â”€â”€ ğŸ“ processed/               # Datos procesados
â”‚       â”œâ”€â”€ ğŸ“ bronze/              # Archivos Parquet
â”‚       â”œâ”€â”€ ğŸ“ silver/              # Datos limpios
â”‚       â””â”€â”€ ğŸ“ gold/                # Datos analÃ­ticos
â”œâ”€â”€ ğŸ“ logs/                        # Logs del sistema
â”œâ”€â”€ ğŸ“ test/                        # Logs del sistema
        â”œâ”€â”€ ğŸ“ unit_testing/        # Pruebas unitarias para los diferentes scripts (Sin terminar)
â”œâ”€â”€ ğŸ“„ requirements.txt             # Dependencias Python
â”œâ”€â”€ ğŸ“„ .env                         # Variables de entorno
â””â”€â”€ ğŸ“„ README.md                    # Esta documentaciÃ³n
```

## ğŸ“ Arquitectura del Proyecto

![Diagrama de Arquitectura](assets/architecture.png)

## ğŸ“ Estructura de la base de datos

![Diagrama de Tablas](assets/db.png)

ğŸ“Š transactions (Tabla Principal)
Almacena todas las transacciones procesadas desde los archivos CSV. Cada registro representa una transacciÃ³n individual con informaciÃ³n de precio, usuario, timestamp y metadatos de procesamiento. Incluye Ã­ndices optimizados para consultas frecuentes por timestamp, precio, usuario y archivo fuente.

ğŸ“¦ batch_metadata (Control de Procesamiento)
Registra informaciÃ³n de cada micro-batch procesado durante la ingesta. Mantiene el estado del procesamiento, nÃºmero de filas, tiempos de ejecuciÃ³n y capturas de estadÃ­sticas para auditorÃ­a y monitoreo del pipeline.

ğŸ” stats_verification (AuditorÃ­a de EstadÃ­sticas)
Almacena los resultados de las verificaciones automÃ¡ticas que comparan las estadÃ­sticas incrementales con las consultas directas a la base de datos. Garantiza la integridad y precisiÃ³n del motor de estadÃ­sticas O(1).

## ğŸ§ª VerificaciÃ³n del Reto

El sistema implementa verificaciÃ³n automÃ¡tica del cumplimiento de requerimientos:

### Punto 1: Descarga de Datos âœ…

- Descarga automÃ¡tica desde Google Drive
- VerificaciÃ³n de 6 archivos CSV
- ValidaciÃ³n de integridad

### Punto 2: Procesamiento sin Cargar Todo âœ…

- Micro-batches de 1,000 filas
- ConversiÃ³n incremental CSV â†’ Parquet
- OptimizaciÃ³n de memoria

### Punto 3: EstadÃ­sticas en EjecuciÃ³n âœ…

- Motor incremental O(1)
- NO recalcula desde base de datos
- VerificaciÃ³n automÃ¡tica vs consultas SQL

### Punto 4: Validation.csv âœ…

- Procesamiento separado de validation.csv
- EstadÃ­sticas antes/despuÃ©s
- DetecciÃ³n automÃ¡tica de cambios

# Demo versiÃ³n grÃ¡fica Streamlit

![Demo Streamlit](assets/str1.png)

![Demo Streamlit](assets/str2.png)

![Demo Streamlit](assets/str3.png)

![Demo Streamlit](assets/str4.png)

![Demo Streamlit](assets/str5.png)

