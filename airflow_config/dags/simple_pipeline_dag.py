# airflow_config/dags/simple_pipeline_dag.py
"""
ðŸš€ DAG SIMPLIFICADO - EJECUTA SCRIPTS DIRECTAMENTE
=================================================
En lugar de reimplementar la lÃ³gica, simplemente ejecuta los scripts que ya funcionan.
"""

from datetime import datetime, timedelta
from pathlib import Path

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago

# ==========================================
# CONFIGURACIÃ“N DEL DAG
# ==========================================

default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'simple_pipeline_execution',
    default_args=default_args,
    description='Pipeline simplificado - Ejecuta scripts directamente',
    schedule_interval=None,  # Solo manual
    catchup=False,
    max_active_runs=1,
    tags=['simple', 'scripts', 'production'],
)

# ==========================================
# CONFIGURACIÃ“N DE PATHS
# ==========================================

# Detectar ruta del proyecto automÃ¡ticamente
PROJECT_ROOT = Path(__file__).parent.parent.parent
VENV_PYTHON = PROJECT_ROOT / ".venv" / "bin" / "python"

# ==========================================
# TAREAS COMO BASH OPERATORS - Â¡SÃšPER SIMPLE!
# ==========================================

start_task = DummyOperator(
    task_id='start_pipeline',
    dag=dag,
)

# 1. Descargar datos (si es necesario)
download_task = BashOperator(
    task_id='download_data',
    bash_command=f"""
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    python3 src/data_flow/download_data.py
    """,
    dag=dag,
)

# 2. Convertir CSV a Bronze (Parquet)
bronze_task = BashOperator(
    task_id='convert_to_bronze',
    bash_command=f"""
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    python3 src/data_flow/bronze_converter.py
    """,
    dag=dag,
)

# 3. Â¡EJECUTAR EL PIPELINE PRINCIPAL DIRECTAMENTE!
pipeline_task = BashOperator(
    task_id='run_main_pipeline',
    bash_command=f"""
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    python3 src/pipeline/data_ingestion.py
    """,
    dag=dag,
)

# 4. Generar reporte simple
report_task = BashOperator(
    task_id='generate_report',
    bash_command=f"""
    cd {PROJECT_ROOT}
    echo "ðŸ“Š REPORTE DE PIPELINE - $(date)" > logs/pipeline_report_$(date +%Y%m%d).txt
    echo "=================================" >> logs/pipeline_report_$(date +%Y%m%d).txt
    echo "âœ… Pipeline ejecutado exitosamente" >> logs/pipeline_report_$(date +%Y%m%d).txt
    echo "ðŸ“ Archivos Bronze:" >> logs/pipeline_report_$(date +%Y%m%d).txt
    ls -la data/processed/bronze/ >> logs/pipeline_report_$(date +%Y%m%d).txt
    echo "ðŸ“Š Base de datos:" >> logs/pipeline_report_$(date +%Y%m%d).txt
    ls -la data/pipeline.db >> logs/pipeline_report_$(date +%Y%m%d).txt
    echo "ðŸ“„ Reporte guardado en logs/pipeline_report_$(date +%Y%m%d).txt"
    """,
    dag=dag,
)

end_task = DummyOperator(
    task_id='pipeline_completed',
    dag=dag,
)

# ==========================================
# DEPENDENCIAS - FLUJO LINEAL SIMPLE
# ==========================================

start_task >> download_task >> bronze_task >> pipeline_task >> report_task >> end_task

# ==========================================
# OPCIÃ“N ALTERNATIVA: DAG AUN MÃS SIMPLE
# ==========================================

# Si quieres hacer TODO en una sola tarea:
simple_dag = DAG(
    'ultra_simple_pipeline',
    default_args=default_args,
    description='Pipeline ultra simplificado - Todo en una tarea',
    schedule_interval=None,
    catchup=False,
    max_active_runs=1,
    tags=['ultra-simple', 'one-task'],
)

# Una sola tarea que ejecuta todo el pipeline
all_in_one_task = BashOperator(
    task_id='run_complete_pipeline',
    bash_command=f"""
    echo "ðŸš€ Iniciando pipeline completo..."
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    
    echo "ðŸ“¥ Descargando datos..."
    python3 src/data_flow/download_data.py || echo "âš ï¸ Descarga fallÃ³ o datos ya existen"
    
    echo "ðŸ¥‰ Convirtiendo a Bronze..."
    python3 src/data_flow/bronze_converter.py || echo "âš ï¸ ConversiÃ³n fallÃ³ o archivos ya existen"
    
    echo "ðŸš€ Ejecutando pipeline principal..."
    python3 src/pipeline/data_ingestion.py
    
    echo "ðŸ“Š Generando reporte..."
    mkdir -p logs
    echo "Pipeline completado exitosamente en $(date)" > logs/simple_report_$(date +%Y%m%d).txt
    
    echo "âœ… Â¡Pipeline completado!"
    """,
    dag=simple_dag,
)

# ==========================================
# DAG MODULAR CON VALIDACIÃ“N
# ==========================================

modular_dag = DAG(
    'modular_pipeline',
    default_args=default_args,
    description='Pipeline modular con validaciones',
    schedule_interval=None,
    catchup=False,
    max_active_runs=1,
    tags=['modular', 'validated'],
)

# Verificar prerequisites
check_task = BashOperator(
    task_id='check_prerequisites',
    bash_command=f"""
    cd {PROJECT_ROOT}
    echo "ðŸ” Verificando prerequisites..."
    
    # Verificar virtual environment
    if [ ! -f ".venv/bin/activate" ]; then
        echo "âŒ Virtual environment no encontrado"
        exit 1
    fi
    
    # Verificar directorios
    mkdir -p data/raw data/processed/bronze data/processed/silver data/processed/gold logs
    
    # Verificar archivos crÃ­ticos
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    
    echo "ðŸ“¦ Verificando mÃ³dulos Python..."
    python3 -c "
import sys
sys.path.insert(0, 'src')
try:
    from pipeline.data_ingestion import DataIngestionPipeline
    print('âœ… DataIngestionPipeline importado')
except Exception as e:
    print(f'âŒ Error importando: {{e}}')
    exit(1)
"
    
    echo "âœ… Prerequisites verificados"
    """,
    dag=modular_dag,
)

# Ejecutar descarga con validaciÃ³n
download_validated_task = BashOperator(
    task_id='download_with_validation',
    bash_command=f"""
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    
    echo "ðŸ“¥ Ejecutando descarga..."
    python3 -c "
from src.data_flow.download_data import DataDownloader
downloader = DataDownloader()
success, _ = downloader.download_challenge_data()
if success:
    verification = downloader.verify_downloaded_data()
    if verification:
        print('âœ… Descarga y verificaciÃ³n exitosas')
    else:
        print('âŒ Error en verificaciÃ³n')
        exit(1)
else:
    print('âš ï¸ Descarga fallÃ³, verificando archivos existentes...')
    # Verificar si ya existen
    import os
    csv_count = len([f for f in os.listdir('data/raw') if f.endswith('.csv')] if os.path.exists('data/raw') else [])
    if csv_count >= 6:
        print(f'âœ… Encontrados {{csv_count}} archivos CSV existentes')
    else:
        print(f'âŒ Solo se encontraron {{csv_count}} archivos CSV')
        exit(1)
"
    """,
    dag=modular_dag,
)

# Ejecutar Bronze con validaciÃ³n
bronze_validated_task = BashOperator(
    task_id='bronze_with_validation',
    bash_command=f"""
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    
    echo "ðŸ¥‰ Ejecutando conversiÃ³n Bronze..."
    python3 -c "
from src.data_flow.bronze_converter import BronzeConverter
converter = BronzeConverter()
result = converter.convert_all_csv_to_bronze()
if result.get('success', False):
    print(f'âœ… Bronze exitoso: {{result.get(\"converted_files\", 0)}} archivos')
    
    # Verificar archivos generados
    import os
    parquet_count = len([f for f in os.listdir('data/processed/bronze') if f.endswith('.parquet')] if os.path.exists('data/processed/bronze') else [])
    print(f'ðŸ“Š Archivos Parquet generados: {{parquet_count}}')
    
    if parquet_count >= 6:
        print('âœ… ConversiÃ³n Bronze verificada')
    else:
        print('âŒ Archivos Parquet insuficientes')
        exit(1)
else:
    print('âŒ Error en conversiÃ³n Bronze')
    exit(1)
"
    """,
    dag=modular_dag,
)

# Pipeline principal - Â¡EL QUE YA TIENES!
main_pipeline_task = BashOperator(
    task_id='execute_main_pipeline',
    bash_command=f"""
    cd {PROJECT_ROOT}
    source .venv/bin/activate
    export PYTHONPATH="{PROJECT_ROOT}/src:$PYTHONPATH"
    
    echo "ðŸš€ Ejecutando pipeline principal..."
    echo "ðŸ“ Directorio actual: $(pwd)"
    echo "ðŸ Python: $(which python3)"
    echo "ðŸ“Š Archivos Bronze disponibles:"
    ls -la data/processed/bronze/
    
    # Ejecutar el pipeline que YA FUNCIONA
    python3 src/pipeline/data_ingestion.py
    
    echo "ðŸ“Š Verificando resultados..."
    echo "Base de datos generada:"
    ls -la data/pipeline.db
    
    echo "âœ… Pipeline principal completado"
    """,
    dag=modular_dag,
)

# Reporte final detallado
final_report_task = BashOperator(
    task_id='generate_final_report',
    bash_command=f"""
    cd {PROJECT_ROOT}
    
    REPORT_FILE="logs/detailed_report_$(date +%Y%m%d_%H%M%S).txt"
    
    echo "ðŸ“Š REPORTE DETALLADO DEL PIPELINE" > $REPORT_FILE
    echo "=================================" >> $REPORT_FILE
    echo "ðŸ“… Fecha: $(date)" >> $REPORT_FILE
    echo "ðŸ“ Proyecto: {PROJECT_ROOT}" >> $REPORT_FILE
    echo "" >> $REPORT_FILE
    
    echo "ðŸ“‹ ARCHIVOS GENERADOS:" >> $REPORT_FILE
    echo "Bronze layer:" >> $REPORT_FILE
    ls -la data/processed/bronze/ >> $REPORT_FILE
    echo "" >> $REPORT_FILE
    
    echo "Base de datos:" >> $REPORT_FILE
    ls -la data/pipeline.db >> $REPORT_FILE
    echo "" >> $REPORT_FILE
    
    echo "Logs del pipeline:" >> $REPORT_FILE
    ls -la logs/ | tail -5 >> $REPORT_FILE
    echo "" >> $REPORT_FILE
    
    echo "ðŸŽ¯ ESTADO: PIPELINE COMPLETADO EXITOSAMENTE" >> $REPORT_FILE
    
    echo "ðŸ“„ Reporte guardado en: $REPORT_FILE"
    cat $REPORT_FILE
    """,
    dag=modular_dag,
)

# Dependencias para DAG modular
start_modular = DummyOperator(task_id='start_modular', dag=modular_dag)
end_modular = DummyOperator(task_id='end_modular', dag=modular_dag)

start_modular >> check_task >> download_validated_task >> bronze_validated_task >> main_pipeline_task >> final_report_task >> end_modular