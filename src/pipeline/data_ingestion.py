# src/pipeline/data_ingestion.py - VERSI√ìN CORREGIDA
"""
Data Ingestion Pipeline - N√∫cleo del Reto
‚úÖ Bronze (Parquet) ‚Üí Base de Datos + Estad√≠sticas Incrementales
‚úÖ Procesamiento en micro-batches (cumple requerimiento de memoria)
‚úÖ Estad√≠sticas O(1) sin tocar datos ya cargados
‚úÖ Verificaci√≥n final comparando stats incrementales vs consulta directa
‚úÖ FIXED: Filtrado de valores NaN para consistencia BD ‚Üî Stats
"""

import logging
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import json
import sys

# Configurar path para imports
current_dir = Path(__file__).parent
src_dir = current_dir.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

try:
    from pipeline.statistics_engine import IncrementalStatisticsEngine
    from pipeline.database_setup import DatabaseManager
    from config.medallion_config import BRONZE_PATH, EXPECTED_FILE_STEMS
except ImportError as e:
    logging.error(f"Error importando m√≥dulos: {e}")
    logging.error("Aseg√∫rate de ejecutar desde la ra√≠z del proyecto")
    sys.exit(1)

logger = logging.getLogger(__name__)

class DataIngestionPipeline:
    """
    Pipeline principal que implementa los requerimientos del reto:
    
    ‚úÖ Cargar archivos CSV ‚Üí BD (desde Bronze Parquet optimizado)
    ‚úÖ Estad√≠sticas incrementales en tiempo real (count, avg, min, max)
    ‚úÖ NO recalcular desde BD (O(1) vs O(n))
    ‚úÖ Procesamiento en micro-batches (cumple memoria)
    ‚úÖ Verificaci√≥n final stats incrementales vs consulta directa
    ‚úÖ Procesar validation.csv y mostrar cambios
    ‚úÖ FIXED: Filtrar NaN para mantener consistencia BD ‚Üî Stats
    """
    
    def __init__(self, 
                 database_config: Optional[Dict[str, Any]] = None,
                 batch_size: int = 1000,
                 enable_persistence: bool = True,
                 project_root: Optional[str] = None):
        """
        Inicializa el pipeline de ingesta de datos
        
        Args:
            database_config: Configuraci√≥n de BD (None para SQLite por defecto)
            batch_size: Tama√±o de micro-batch para procesamiento
            enable_persistence: Si persistir estad√≠sticas en archivo
            project_root: Ruta ra√≠z del proyecto
        """
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.batch_size = batch_size
        
        # Inicializar componentes
        self.db_manager = DatabaseManager(database_config)
        
        # Configurar persistencia de estad√≠sticas
        persistence_file = None
        if enable_persistence:
            persistence_file = str(self.project_root / "data" / "processed" / "pipeline_statistics.json")
        
        self.stats_engine = IncrementalStatisticsEngine(persistence_file=persistence_file)
        
        # Configurar rutas
        self.bronze_path = self.project_root / "data" / "processed" / "bronze"
        
        # Contadores y metadata
        self.files_processed = []
        self.total_files_to_process = 0
        self.total_batches_processed = 0
        self.pipeline_start_time = None
        self.pipeline_end_time = None
        
        logger.info(f"üîÑ DataIngestionPipeline inicializado")
        logger.info(f"   Micro-batch size: {self.batch_size:,} filas")
        logger.info(f"   Bronze path: {self.bronze_path}")
        logger.info(f"   BD tipo: {self.db_manager.config['type']}")
        logger.info(f"   Persistencia stats: {enable_persistence}")
    
    def process_parquet_file_to_database(self, parquet_file: Path) -> Dict[str, Any]:
        """
        Procesa un archivo Parquet desde Bronze hacia la base de datos
        ‚úÖ CORE IMPLEMENTATION: Micro-batches + Stats incrementales + BD
        ‚úÖ FIXED: Filtrar NaN antes de procesar para mantener consistencia
        
        Args:
            parquet_file: Ruta al archivo Parquet en Bronze
            
        Returns:
            Dict con resultado del procesamiento
        """
        file_name = parquet_file.name
        logger.info(f"\nüîÑ PROCESANDO {file_name} ‚Üí BASE DE DATOS")
        logger.info(f"üìä Modo: Micro-batches de {self.batch_size:,} filas + Estad√≠sticas O(1)")
        
        if not parquet_file.exists():
            logger.error(f"‚ùå Archivo no encontrado: {parquet_file}")
            return {'success': False, 'error': f'File not found: {parquet_file}'}
        
        processing_result = {
            'file_name': file_name,
            'success': False,
            'total_rows': 0,
            'batches_processed': 0,
            'processing_start': datetime.now(),
            'processing_end': None,
            'batch_ids': [],
            'stats_before': self.stats_engine.get_current_stats(),
            'stats_after': None,
            'error': None,
            'rows_filtered': 0  # Nuevo: contar filas filtradas
        }
        
        try:
            # ‚úÖ LEER ARCHIVO PARQUET COMPLETO (ya est√° comprimido ~70% vs CSV)
            logger.info(f"  üìñ Leyendo archivo Parquet comprimido...")
            df = pd.read_parquet(parquet_file)
            original_rows = len(df)
            
            logger.info(f"  üìä Archivo cargado: {original_rows:,} filas")
            
            # ‚úÖ PROCESAR EN MICRO-BATCHES DESDE EL DATAFRAME
            batch_count = 0
            total_processed_rows = 0
            
            # Dividir DataFrame en chunks del tama√±o especificado
            for start_idx in range(0, original_rows, self.batch_size):
                end_idx = min(start_idx + self.batch_size, original_rows)
                chunk_df = df.iloc[start_idx:end_idx].copy()
                
                batch_count += 1
                initial_chunk_size = len(chunk_df)
                
                logger.info(f"  üì¶ Micro-batch {batch_count}: {initial_chunk_size} filas")
                
                # ‚úÖ VALIDAR Y FILTRAR DATOS DEL CHUNK
                if not self._validate_chunk_data(chunk_df, file_name, batch_count):
                    logger.warning(f"‚ö†Ô∏è Saltando batch {batch_count} (validaci√≥n fallida)")
                    continue
                
                final_chunk_size = len(chunk_df)
                rows_filtered_this_batch = initial_chunk_size - final_chunk_size
                processing_result['rows_filtered'] += rows_filtered_this_batch
                
                if final_chunk_size == 0:
                    logger.warning(f"‚ö†Ô∏è Batch {batch_count} vac√≠o despu√©s del filtrado")
                    continue
                
                # 1. ‚úÖ INSERTAR EN BASE DE DATOS
                batch_info = {
                    'source_file': file_name,
                    'batch_number': batch_count,
                    'rows_processed': final_chunk_size,
                    'stats_snapshot': json.dumps(self.stats_engine.get_current_stats())
                }
                
                batch_id = self.db_manager.insert_batch(chunk_df, batch_info)
                processing_result['batch_ids'].append(batch_id)
                
                # 2. ‚úÖ ACTUALIZAR ESTAD√çSTICAS INCREMENTALES (O(1) - SIN CONSULTAR BD)
                # IMPORTANTE: Solo procesar precios v√°lidos (ya filtrados)
                prices = chunk_df['price'].tolist()
                batch_info_for_stats = {
                    'source_file': file_name,
                    'batch_number': batch_count,
                    'batch_id': batch_id
                }
                
                self.stats_engine.update_batch(prices, batch_info_for_stats)
                current_stats = self.stats_engine.get_current_stats()
                
                # 3. ‚úÖ MOSTRAR PROGRESO EN TIEMPO REAL (REQUERIMIENTO DEL RETO)
                logger.info(f"     üíæ BD: ‚úÖ Insertado (batch_id: {batch_id[:8]}...)")
                logger.info(f"     üìä Stats: {self.stats_engine.format_stats()}")
                
                total_processed_rows += final_chunk_size
                
                # Limpiar memoria del chunk
                del chunk_df
                
                # Log progreso cada 5 batches
                if batch_count % 5 == 0:
                    logger.info(f"    üìà Progreso: {total_processed_rows:,} filas v√°lidas procesadas en {batch_count} micro-batches")
            
            # Limpiar DataFrame completo de la memoria
            del df
            
            # Finalizar procesamiento del archivo
            processing_result.update({
                'success': True,
                'total_rows': total_processed_rows,  # Solo filas v√°lidas procesadas
                'batches_processed': batch_count,
                'processing_end': datetime.now(),
                'stats_after': self.stats_engine.get_current_stats()
            })
            
            self.total_batches_processed += batch_count
            
            # ‚úÖ RESUMEN FINAL CON INFO DE FILTRADO
            logger.info(f"‚úÖ {file_name} procesado completamente:")
            logger.info(f"   üìä Filas originales: {original_rows:,}")
            if processing_result['rows_filtered'] > 0:
                logger.info(f"   üßπ Filas filtradas (NaN, etc.): {processing_result['rows_filtered']:,}")
            logger.info(f"   ‚úÖ Filas v√°lidas procesadas: {total_processed_rows:,}")
            logger.info(f"   üì¶ Micro-batches: {batch_count}")
            logger.info(f"   üìà Stats actuales: {self.stats_engine.format_stats()}")
            
            return processing_result
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando {file_name}: {e}")
            processing_result.update({
                'success': False,
                'error': str(e),
                'processing_end': datetime.now()
            })
            return processing_result
    
    def _validate_chunk_data(self, chunk_df: pd.DataFrame, file_name: str, batch_number: int) -> bool:
        """
        Valida los datos de un chunk antes de procesarlo
        ‚úÖ FIXED: Filtrar y limpiar datos problem√°ticos antes de procesar
        
        Args:
            chunk_df: DataFrame del chunk (SE MODIFICA IN-PLACE)
            file_name: Nombre del archivo fuente
            batch_number: N√∫mero del batch
            
        Returns:
            bool: True si los datos son v√°lidos
        """
        import pandas as pd
        import numpy as np
        
        required_columns = ['timestamp', 'price', 'user_id', 'source_file']
        missing_columns = [col for col in required_columns if col not in chunk_df.columns]
        
        if missing_columns:
            logger.error(f"‚ùå {file_name} batch {batch_number}: Faltan columnas {missing_columns}")
            return False
        
        # ‚úÖ CR√çTICO: Contar y filtrar valores NaN en price ANTES de procesar
        initial_count = len(chunk_df)
        nan_prices = chunk_df['price'].isna().sum()
        
        if nan_prices > 0:
            logger.warning(f"‚ö†Ô∏è {file_name} batch {batch_number}: {nan_prices} precios NaN encontrados - FILTRANDO")
            # FILTRAR filas con precios NaN del DataFrame (IN-PLACE)
            chunk_df.dropna(subset=['price'], inplace=True)
            chunk_df.reset_index(drop=True, inplace=True)
            final_count = len(chunk_df)
            logger.info(f"üßπ {file_name} batch {batch_number}: Filtrado {initial_count - final_count} filas con NaN")
        
        # Filtrar precios <= 0 tambi√©n
        invalid_prices = chunk_df[chunk_df['price'] <= 0]
        if len(invalid_prices) > 0:
            logger.warning(f"‚ö†Ô∏è {file_name} batch {batch_number}: {len(invalid_prices)} precios ‚â§0 encontrados - FILTRANDO")
            chunk_df = chunk_df[chunk_df['price'] > 0]
            chunk_df.reset_index(drop=True, inplace=True)
        
        # Validar nulos en otros campos cr√≠ticos
        null_counts = chunk_df[required_columns].isnull().sum()
        total_nulls = null_counts.sum()
        if total_nulls > 0:
            logger.warning(f"‚ö†Ô∏è {file_name} batch {batch_number}: {total_nulls} valores nulos en otros campos")
        
        # Verificar que el chunk a√∫n tiene datos despu√©s del filtrado
        if len(chunk_df) == 0:
            logger.warning(f"‚ö†Ô∏è {file_name} batch {batch_number}: Chunk vac√≠o despu√©s del filtrado")
            return False
        
        return True
    
    def process_all_bronze_files(self, exclude_validation: bool = True) -> Dict[str, Any]:
        """
        Procesa todos los archivos Parquet de Bronze hacia la base de datos
        ‚úÖ IMPLEMENTA REQUERIMIENTO: Cargar todos los CSV (desde Bronze optimizado)
        
        Args:
            exclude_validation: Si excluir validation.csv (procesar por separado)
            
        Returns:
            Dict con resultado del procesamiento completo
        """
        self.pipeline_start_time = datetime.now()
        
        logger.info("üöÄ INICIANDO PIPELINE DE INGESTA DE DATOS")
        logger.info("=" * 60)
        logger.info(f"‚ö° Configuraci√≥n: Micro-batches de {self.batch_size:,} filas")
        logger.info(f"üóÑÔ∏è Base de datos: {self.db_manager.config['type']}")
        logger.info(f"üìä Estad√≠sticas: Incrementales O(1) (NO recalcula desde BD)")
        
        # Obtener archivos a procesar
        files_to_process = []
        for file_stem in EXPECTED_FILE_STEMS:
            if exclude_validation and file_stem == 'validation':
                continue
            
            parquet_file = self.bronze_path / f"{file_stem}.parquet"
            if parquet_file.exists():
                files_to_process.append(parquet_file)
            else:
                logger.warning(f"‚ö†Ô∏è Archivo Bronze faltante: {parquet_file.name}")
        
        if not files_to_process:
            logger.error("‚ùå No se encontraron archivos Bronze para procesar")
            return {'success': False, 'error': 'No Bronze files found'}
        
        self.total_files_to_process = len(files_to_process)
        logger.info(f"üìã Archivos a procesar: {self.total_files_to_process}")
        for file in files_to_process:
            logger.info(f"   üìÑ {file.name}")
        
        # Mostrar estad√≠sticas iniciales
        initial_stats = self.stats_engine.get_current_stats()
        logger.info(f"\nüìä ESTAD√çSTICAS INICIALES:")
        logger.info(f"   {self.stats_engine.format_stats()}")
        
        # Procesar archivos secuencialmente (cumple requerimiento de memoria)
        pipeline_result = {
            'success': True,
            'files_processed': [],
            'files_failed': [],
            'total_files': len(files_to_process),
            'total_rows_processed': 0,
            'total_batches_processed': 0,
            'total_rows_filtered': 0,  # Nuevo: total de filas filtradas
            'processing_start': self.pipeline_start_time,
            'processing_end': None,
            'initial_stats': initial_stats,
            'final_stats': None,
            'verification_result': None
        }
        
        # ‚úÖ PROCESAMIENTO SECUENCIAL - UN ARCHIVO A LA VEZ
        for i, parquet_file in enumerate(files_to_process, 1):
            logger.info(f"\nüìÑ ARCHIVO {i}/{len(files_to_process)}: {parquet_file.name}")
            logger.info("-" * 40)
            
            file_result = self.process_parquet_file_to_database(parquet_file)
            
            if file_result['success']:
                pipeline_result['files_processed'].append(file_result)
                pipeline_result['total_rows_processed'] += file_result['total_rows']
                pipeline_result['total_batches_processed'] += file_result['batches_processed']
                pipeline_result['total_rows_filtered'] += file_result.get('rows_filtered', 0)
                
                logger.info(f"‚úÖ {parquet_file.name}: {file_result['total_rows']:,} filas en {file_result['batches_processed']} batches")
            else:
                pipeline_result['files_failed'].append(file_result)
                logger.error(f"‚ùå {parquet_file.name}: {file_result.get('error', 'Unknown error')}")
        
        # Finalizar pipeline
        self.pipeline_end_time = datetime.now()
        pipeline_result['processing_end'] = self.pipeline_end_time
        pipeline_result['final_stats'] = self.stats_engine.get_current_stats()
        
        # ‚úÖ VERIFICACI√ìN CR√çTICA: Comparar estad√≠sticas incrementales vs consulta directa
        logger.info(f"\nüîç VERIFICACI√ìN FINAL: ESTAD√çSTICAS INCREMENTALES VS BASE DE DATOS")
        logger.info("=" * 70)
        
        try:
            db_stats = self.db_manager.get_database_statistics()
            comparison_result = self.stats_engine.compare_with_database_stats(db_stats)
            pipeline_result['verification_result'] = comparison_result
            
            # Guardar verificaci√≥n en BD para auditor√≠a
            self.db_manager.save_verification_result(
                pipeline_result['final_stats'],
                db_stats,
                comparison_result
            )
            
            if comparison_result['overall_match']:
                logger.info("üéâ VERIFICACI√ìN EXITOSA: Las estad√≠sticas incrementales coinciden exactamente con la BD")
            else:
                logger.error("‚ùå VERIFICACI√ìN FALLIDA: Hay diferencias entre estad√≠sticas incrementales y BD")
                pipeline_result['success'] = False
        
        except Exception as e:
            logger.error(f"‚ùå Error en verificaci√≥n final: {e}")
            pipeline_result['verification_result'] = {'error': str(e)}
        
        # Resumen final
        self._print_pipeline_summary(pipeline_result)
        
        return pipeline_result
    
    def process_validation_file(self) -> Dict[str, Any]:
        """
        Procesa validation.csv y muestra c√≥mo cambian las estad√≠sticas
        ‚úÖ IMPLEMENTA REQUERIMIENTO: Ejecutar validation.csv y mostrar cambios
        
        Returns:
            Dict con resultado del procesamiento de validaci√≥n
        """
        logger.info("\nüß™ PROCESANDO ARCHIVO DE VALIDACI√ìN")
        logger.info("=" * 45)
        
        validation_file = self.bronze_path / "validation.parquet"
        
        if not validation_file.exists():
            logger.error(f"‚ùå Archivo de validaci√≥n no encontrado: {validation_file}")
            return {'success': False, 'error': 'Validation file not found'}
        
        # Capturar estad√≠sticas ANTES de validation
        stats_before = self.stats_engine.get_current_stats()
        logger.info(f"üìä ESTAD√çSTICAS ANTES DE VALIDATION:")
        logger.info(f"   {self.stats_engine.format_stats()}")
        
        # Procesar validation.csv
        validation_result = self.process_parquet_file_to_database(validation_file)
        
        if not validation_result['success']:
            logger.error(f"‚ùå Error procesando validation: {validation_result.get('error')}")
            return validation_result
        
        # Capturar estad√≠sticas DESPU√âS de validation
        stats_after = self.stats_engine.get_current_stats()
        logger.info(f"\nüìä ESTAD√çSTICAS DESPU√âS DE VALIDATION:")
        logger.info(f"   {self.stats_engine.format_stats()}")
        
        # ‚úÖ MOSTRAR CAMBIOS (REQUERIMIENTO DEL RETO)
        logger.info(f"\nüìà CAMBIOS DETECTADOS:")
        logger.info("-" * 25)
        
        count_change = stats_after['count'] - stats_before['count']
        avg_change = stats_after['avg'] - stats_before['avg'] if stats_before['count'] > 0 else stats_after['avg']
        
        logger.info(f"   Filas agregadas: +{count_change:,}")
        logger.info(f"   Cambio en promedio: {avg_change:+.2f} (${stats_before['avg']:.2f} ‚Üí ${stats_after['avg']:.2f})")
        
        if stats_after['min'] < stats_before.get('min', float('inf')):
            logger.info(f"   üîΩ Nuevo m√≠nimo: ${stats_after['min']:.2f} (antes: ${stats_before.get('min', 0):.2f})")
        
        if stats_after['max'] > stats_before.get('max', float('-inf')):
            logger.info(f"   üîº Nuevo m√°ximo: ${stats_after['max']:.2f} (antes: ${stats_before.get('max', 0):.2f})")
        
        # Verificaci√≥n final despu√©s de validation
        logger.info(f"\nüîç VERIFICACI√ìN FINAL DESPU√âS DE VALIDATION:")
        db_stats = self.db_manager.get_database_statistics()
        comparison_result = self.stats_engine.compare_with_database_stats(db_stats)
        
        validation_result.update({
            'stats_before': stats_before,
            'stats_after': stats_after,
            'changes': {
                'count_added': count_change,
                'avg_change': avg_change,
                'new_min': stats_after['min'] < stats_before.get('min', float('inf')),
                'new_max': stats_after['max'] > stats_before.get('max', float('-inf'))
            },
            'final_verification': comparison_result
        })
        
        if comparison_result['overall_match']:
            logger.info("‚úÖ Verificaci√≥n final exitosa despu√©s de validation")
        else:
            logger.error("‚ùå Verificaci√≥n final fallida despu√©s de validation")
        
        return validation_result
    
    def _print_pipeline_summary(self, pipeline_result: Dict[str, Any]):
        """
        Imprime resumen completo del pipeline
        """
        duration = (self.pipeline_end_time - self.pipeline_start_time).total_seconds()
        
        logger.info(f"\nüéØ RESUMEN FINAL DEL PIPELINE")
        logger.info("=" * 40)
        logger.info(f"‚è±Ô∏è Duraci√≥n total: {duration:.2f} segundos")
        logger.info(f"üìÑ Archivos procesados: {len(pipeline_result['files_processed'])}/{pipeline_result['total_files']}")
        logger.info(f"üìä Total filas procesadas: {pipeline_result['total_rows_processed']:,}")
        
        # ‚úÖ NUEVO: Mostrar informaci√≥n de filtrado
        if pipeline_result.get('total_rows_filtered', 0) > 0:
            logger.info(f"üßπ Total filas filtradas (NaN/inv√°lidas): {pipeline_result['total_rows_filtered']:,}")
        
        logger.info(f"üì¶ Total micro-batches: {pipeline_result['total_batches_processed']:,}")
        logger.info(f"‚ö° Rendimiento: {pipeline_result['total_rows_processed']/duration:.0f} filas/segundo")
        
        # Estad√≠sticas finales
        logger.info(f"\n{self.stats_engine.format_detailed_stats()}")
        
        # Resumen de BD
        try:
            batch_summary = self.db_manager.get_batch_summary()
            logger.info(f"\nüóÑÔ∏è RESUMEN DE BASE DE DATOS:")
            logger.info(f"   Total batches en BD: {batch_summary.get('total_batches', 0)}")
            logger.info(f"   Batches exitosos: {batch_summary.get('completed_batches', 0)}")
            logger.info(f"   Tasa de √©xito: {batch_summary.get('success_rate', 0):.1f}%")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudo obtener resumen de BD: {e}")
        
        # Performance del motor de estad√≠sticas
        perf_metrics = self.stats_engine.get_performance_metrics()
        logger.info(f"\n‚ö° PERFORMANCE DE ESTAD√çSTICAS:")
        logger.info(f"   Eficiencia: {perf_metrics.get('processing_efficiency', 'N/A')}")
        logger.info(f"   Uso de memoria: {perf_metrics.get('memory_usage', 'N/A')}")
        
        # Estado final
        verification = pipeline_result.get('verification_result', {})
        if verification.get('overall_match'):
            logger.info(f"\nüéâ PIPELINE COMPLETADO EXITOSAMENTE")
            logger.info(f"‚úÖ Estad√≠sticas incrementales verificadas correctamente")
        else:
            logger.error(f"\n‚ùå PIPELINE COMPLETADO CON ERRORES")
            logger.error(f"‚ùå Verificaci√≥n de estad√≠sticas fallida")
    
    def run_complete_pipeline(self) -> Dict[str, Any]:
        """
        Ejecuta el pipeline completo: archivos principales + validation
        ‚úÖ IMPLEMENTA TODOS LOS REQUERIMIENTOS DEL RETO
        
        Returns:
            Dict con resultado completo
        """
        logger.info("üöÄ EJECUTANDO PIPELINE COMPLETO - RETO DE INGENIER√çA DE DATOS")
        logger.info("=" * 70)
        
        complete_result = {
            'main_pipeline': None,
            'validation_pipeline': None,
            'overall_success': False,
            'execution_start': datetime.now(),
            'execution_end': None
        }
        
        try:
            # 1. Procesar archivos principales (2012-1 a 2012-5)
            logger.info("üìã FASE 1: PROCESANDO ARCHIVOS PRINCIPALES (2012-1 a 2012-5)")
            main_result = self.process_all_bronze_files(exclude_validation=True)
            complete_result['main_pipeline'] = main_result
            
            if not main_result['success']:
                logger.error("‚ùå Error en procesamiento de archivos principales")
                return complete_result
            
            # 2. Procesar validation.csv
            logger.info("\nüìã FASE 2: PROCESANDO ARCHIVO DE VALIDACI√ìN")
            validation_result = self.process_validation_file()
            complete_result['validation_pipeline'] = validation_result
            
            if not validation_result['success']:
                logger.error("‚ùå Error en procesamiento de validation")
                return complete_result
            
            complete_result['overall_success'] = True
            
            # Resumen final completo
            logger.info(f"\nüéâ PIPELINE COMPLETO EJECUTADO EXITOSAMENTE")
            logger.info("=" * 50)
            
            total_rows = main_result['total_rows_processed'] + validation_result['total_rows']
            total_batches = main_result['total_batches_processed'] + validation_result['batches_processed']
            
            logger.info(f"üìä TOTALES FINALES:")
            logger.info(f"   Archivos procesados: {main_result['total_files'] + 1}")
            logger.info(f"   Filas totales: {total_rows:,}")
            logger.info(f"   Micro-batches totales: {total_batches:,}")
            logger.info(f"   Verificaci√≥n final: {'‚úÖ EXITOSA' if validation_result.get('final_verification', {}).get('overall_match') else '‚ùå FALLIDA'}")
            
            return complete_result
            
        except Exception as e:
            logger.error(f"‚ùå Error en pipeline completo: {e}")
            complete_result['error'] = str(e)
            return complete_result
        
        finally:
            complete_result['execution_end'] = datetime.now()
    
    def cleanup(self):
        """
        Limpia recursos del pipeline
        """
        try:
            self.db_manager.close()
            logger.info("üßπ Recursos del pipeline liberados")
        except Exception as e:
            logger.error(f"‚ùå Error en cleanup: {e}")


def main():
    """
    Funci√≥n principal para ejecutar el pipeline completo
    """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("üöÄ INICIANDO DATA INGESTION PIPELINE")
    logger.info("Implementando requerimientos completos del reto de ingenier√≠a de datos")
    
    pipeline = None
    try:
        # Crear y ejecutar pipeline
        pipeline = DataIngestionPipeline(
            batch_size=1000,  # Micro-batches de 1000 filas
            enable_persistence=True
        )
        
        # Ejecutar pipeline completo
        result = pipeline.run_complete_pipeline()
        
        if result['overall_success']:
            logger.info("üéâ RETO COMPLETADO EXITOSAMENTE")
            return 0
        else:
            logger.error("‚ùå RETO COMPLETADO CON ERRORES")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico: {e}")
        return 1
    
    finally:
        if pipeline:
            pipeline.cleanup()


if __name__ == "__main__":
    import sys
    exit_code = main()
    sys.exit(exit_code)